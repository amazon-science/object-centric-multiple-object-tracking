defaults:
  - /experiment/_output_path
  - /training_config
  - /dataset: cater
  - /plugins/optimization@plugins.optimize_parameters
  - /plugins/random_strided_window@plugins.02_random_strided_window         # Used during training.
  - /plugins/multi_element_preprocessing@plugins.03_preprocessing
  - /optimizers/adam@plugins.optimize_parameters.optimizer
  - /lr_schedulers/cosine_annealing@plugins.optimize_parameters.lr_scheduler
  - /experiment/SAVi/_cater_bbox_mot_preprocessing
#  - /metrics/three_d_iou@evaluation_metrics.iou
#  - /metrics/mot_metric@evaluation_metrics.mot
  - /metrics/ari_metric@evaluation_metrics.ari
  - _self_



load_checkpoint: outputs/SAVi/savi/2023-02-20_23-49-54/checkpoints/epoch=18-step=1064.ckpt

trainer:
  gpus: 8
  gradient_clip_val: 0.05
  gradient_clip_algorithm: "norm"
  max_epochs: null
  max_steps: 2000005
  strategy: 'ddp'
  callbacks:
    - _target_: pytorch_lightning.callbacks.LearningRateMonitor
      logging_interval: "step"

dataset:
  num_workers: 4
  batch_size: 30

models:
  _target_: ocl.models.SAVi
  conditioning:
    _target_: ocl.conditioning.LearntConditioning
    n_slots: 11
    object_dim: 128

  feature_extractor:
    # Use the smaller verion of the feature extractor architecture.
    _target_: ocl.feature_extractors.SAViFeatureExtractor
    larger_input_arch: False

  perceptual_grouping:
    _target_: ocl.perceptual_grouping.SlotAttentionGrouping
    feature_dim: 32
    object_dim: ${models.conditioning.object_dim}
    iters: 2
    kvq_dim: 128
    use_projection_bias: false
    positional_embedding:
      _target_: ocl.utils.Sequential
      _args_:
        - _target_: ocl.utils.SoftPositionEmbed
          n_spatial_dims: 2
          feature_dim: 32
          savi_style: true
        - _target_: ocl.neural_networks.build_two_layer_mlp
          input_dim: 32
          output_dim: 32
          hidden_dim: 64
          initial_layer_norm: true
    ff_mlp: null

  decoder:
    _target_: ocl.decoding.SlotAttentionDecoder
    decoder:
      _target_: ocl.decoding.get_savi_decoder_backbone
      object_dim: ${models.perceptual_grouping.object_dim}
      larger_input_arch: False
    positional_embedding:
      _target_: ocl.utils.SoftPositionEmbed
      n_spatial_dims: 2
      feature_dim: ${models.perceptual_grouping.object_dim}
      cnn_channel_order: true
      savi_style: true

  transition_model:
    _target_: torch.nn.Identity

losses:
  mse:
    _target_: ocl.losses.ReconstructionLoss
    loss_type: mse_sum
    input_path: decoder.reconstruction
    target_path: input.image

plugins:
  optimize_parameters:
    optimizer:
      lr: 0.0001
    lr_scheduler:
      T_max: 200000
      eta_min: 0.0
      warmup_steps: 0
  02_random_strided_window:
    n_consecutive_frames: 6
    training_fields:
      - image
    evaluation_fields: []

visualizations:
  input:
    _target_: ocl.visualizations.Video
    denormalization: null
    video_path: input.image
  reconstruction:
    _target_: ocl.visualizations.Video
    denormalization: ${..input.denormalization}
    video_path: decoder.reconstruction
  objects:
    _target_: ocl.visualizations.VisualObject
    denormalization: ${..input.denormalization}
    object_path: decoder.object_reconstructions
    mask_path: decoder.masks_eval
  objectmot:
    _target_: ocl.visualizations.ObjectMOT
    n_clips: 5
    denormalization: null
    video_path: input.image
    mask_path: tracks

evaluation_metrics:
  ari:
    prediction_path: decoder.masks
    target_path: input.mask


